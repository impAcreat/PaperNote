# AI Basic

****

### dropout：

* 作用：在神经网络训练过程中用于防止过拟合的技术。在每个训练步骤中，网络中的每个节点都有一定的概率被临时删除，即在当前步骤中不参与前向和反向传播。这样可以防止网络过度依赖某些特定的神经元，从而提高模型的泛化能力。
* 在训练时dropout，在eval时将dropout设置为0，关闭该功能

****



### 梯度消失

* **出现原因**：在反向传播（Backpropagation）过程中，梯度通过链式法则从输出层逐层传递到输入层（梯度是通过每一层的梯度相乘得到的）。当网络层数很多时，这些梯度的乘积很容易变得非常小，导致早期层的梯度接近于零：如下公式，当每个梯度都小于1时，连乘会让值梯度越来越小

$$
\frac{\partial L}{\partial w}=\frac{\partial L}{\partial z_{L}} \frac{\partial z_{L}}{\partial z_{L-1}} \cdots \frac{\partial z_{2}}{\partial z_{1}} \frac{\partial z_{1}}{\partial w}
$$

* **影响**：早期层的权重几乎不更新，从而使训练过程变得非常缓慢，甚至停止

* **特点**：

  * 在sigmoid和tanh上尤为明显：

  > $$
  > sigmoid = \frac{1}{1+e^{-x}} \\
  > tanh = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
  > $$
  >
  > 这2种激活函数的特点在于，当 x 值非常大或非常小时，值接近0或1，导数则会接近于0

* 解决方法：
  * ReLU，ELU
  * 残差连接
  * 权重初始化：Xavier